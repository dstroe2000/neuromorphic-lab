# SpikeCore Lab -- Derived Product Specification

**Generated:** 2026-02-17
**Source:** Codebase analysis of `neuromorphic-lab/`
**Methodology:** Six-analyst council synthesis (Feature Detective, Domain Analyst, Behavior Analyst, Infrastructure Analyst, Documentation Miner, Workflow Analyst)

---

## Table of Contents

1. [Overview](#1-overview)
2. [Epics, Features, and User Stories](#2-epics-features-and-user-stories)
3. [Entity Glossary](#3-entity-glossary)
4. [User Roles](#4-user-roles)
5. [Non-Functional Requirements](#5-non-functional-requirements)
6. [Workflow Maps](#6-workflow-maps)
7. [Deferred Work (Phase 2)](#7-deferred-work-phase-2)
8. [Traceability Matrix](#8-traceability-matrix)
9. [Coverage Assessment](#9-coverage-assessment)

---

## 1. Overview

SpikeCore Lab is a Python library and Jupyter notebook environment providing a complete neuromorphic compilation pipeline. It compiles trained neural network models into assembly language for a fictional neuromorphic hardware target called "SpikeCore" -- architecturally modeled after Intel Loihi -- simulates execution on virtual hardware, and visualizes results.

The system offers two execution paths:

- **TVM BYOC path** (requires TVM / Docker): Full compilation through TVM's Bring Your Own Codegen framework with Relay IR transformations, quantization passes, graph partitioning, and codegen callbacks.
- **Standalone path** (no TVM required): Manual quantization and direct SpikeCore code generation from PyTorch weight dictionaries.

The reference model is a 2-layer MLP for MNIST classification (784 -> 64 -> 10). The system serves as an educational platform and compilation pipeline demonstration, bridging the gap between ML framework development and hardware-targeted code generation.

**Single user role:** ML Engineer / Researcher.

---

## 2. Epics, Features, and User Stories

### Epic 1: Compile Neural Networks to Neuromorphic Assembly

**Goal:** An ML Engineer wants to transform a trained neural network into executable neuromorphic assembly so that the model can run on SpikeCore hardware.

---

#### Feature 1.1: Standalone Compilation from PyTorch State Dict

Compile a PyTorch model's weights and layer configuration into SpikeCore instructions without TVM.

##### US-1.1.1: Compile Single Hidden Layer with ReLU Activation

**Description**
- As an ML Engineer
- I want to compile a single dense layer with ReLU activation to SpikeCore assembly
- So that I can generate neuromorphic instructions for hidden layers without installing TVM

**Acceptance Criteria**
- [VERIFIED] Given a state dict with float32 weight matrix and layer config specifying `activation="relu"`, when `compile_from_torch()` is called, then for each output neuron the program contains one ACC instruction, one FIRE instruction, and one LEAK instruction
  - *BA-022: test_single_layer_with_relu verifies 10 ACC and 10 FIRE for a (10, 4) layer*
- [VERIFIED] Given a compiled program, when the last instruction is examined, then its opcode is HALT
  - *BA-022: asserts `program[-1].opcode == Opcode.HALT`*
- [VERIFIED] Given a FIRE instruction in the compiled program, when the shift operand is inspected, then shift equals `ceil(log2(fan_in)) + 7`
  - *BA-023: test_single_layer_with_relu implicitly validates via compile_from_torch*
- [INFERRED] Given a weight matrix of shape `(out_features, in_features)`, when compiling, then core IDs are assigned sequentially from 0 to `out_features - 1`

**Evidence:** FD-001, DA-016, DA-017, BA-022, BA-023, BA-024
**Code:** `spikecore/byoc_codegen.py` lines 155-215

---

##### US-1.1.2: Compile Output Layer without Activation

**Description**
- As an ML Engineer
- I want to compile a dense output layer (no activation) to SpikeCore assembly
- So that the final classification layer produces raw accumulator scores rather than spiking outputs

**Acceptance Criteria**
- [VERIFIED] Given a layer config with `activation=None`, when `compile_from_torch()` is called, then only ACC instructions are generated (no FIRE, no LEAK)
  - *BA-022: test_output_layer_no_activation verifies `fire_count == 0` and `acc_count == 5`*
- [VERIFIED] Given the compiled program, when inspected, then it ends with HALT
  - *BA-024: verifies HALT termination*

**Evidence:** FD-001, DA-016, BA-022
**Code:** `spikecore/byoc_codegen.py` lines 196-214

---

##### US-1.1.3: Compile Multi-Layer MLP

**Description**
- As an ML Engineer
- I want to compile a multi-layer MLP (e.g., 784 -> 64 -> 10) to SpikeCore assembly
- So that a complete inference pipeline is mapped to neuromorphic hardware

**Acceptance Criteria**
- [VERIFIED] Given a two-layer MLP with hidden ReLU and output (no activation), when compiled, then core IDs are sequential across layers (layer 0 uses cores 0..N-1, layer 1 uses cores N..N+M-1)
  - *BA-022: test_two_layer_mlp verifies `acc_cores == list(range(8)) + list(range(8, 11))`*
- [VERIFIED] Given a two-layer compilation, when quantized weights are returned, then the list has length 2 with correct shapes
  - *BA-022: test_two_layer_mlp asserts `len(q_weights) == 2`*
- [VERIFIED] Given an MNIST-sized MLP (784 -> 64 -> 10), when compiled via `compile_nn_to_spikecore()`, then weight shapes are preserved: `(64, 784)` and `(10, 64)`
  - *BA-029: test_mnist_mlp_shape verifies exact shapes*

**Evidence:** FD-001, FD-002, DA-015, BA-022, BA-029, BA-030
**Code:** `spikecore/byoc_codegen.py` lines 155-259

---

#### Feature 1.2: Higher-Level Compilation from Weight/Bias Arrays

Compile neural network layers to SpikeCore assembly using raw weight and bias matrices directly, without requiring PyTorch state_dict format.

##### US-1.2.1: Compile from Raw Weight and Bias Matrices

**Description**
- As an ML Engineer
- I want to compile a network by providing lists of weight matrices, bias vectors, and activation types
- So that I can use the compiler without conforming to PyTorch state_dict naming conventions

**Acceptance Criteria**
- [VERIFIED] Given weight matrices, biases, and activations lists, when `compile_nn_to_spikecore()` is called, then it returns a tuple of (program, quantized_weights, quantized_biases)
  - *BA-029: test_mnist_mlp_shape validates the full return signature*
- [VERIFIED] Given a 784 -> 64 -> 10 MLP, when compiled, then the program ends with HALT and produces quantized weights of correct shapes
  - *BA-029, BA-030: verify structure and shape preservation*

**Evidence:** FD-002, BA-029, BA-030
**Code:** `spikecore/byoc_codegen.py` lines 218-259

---

#### Feature 1.3: TVM BYOC Target Registration

Register SpikeCore as a recognized external codegen target within TVM's compilation framework.

##### US-1.3.1: Register SpikeCore BYOC Target

**Description**
- As an ML Engineer with TVM installed
- I want to register "spikecore" as a BYOC target in TVM
- So that TVM's AnnotateTarget pass recognizes SpikeCore-compatible subgraphs

**Acceptance Criteria**
- [INFERRED] Given TVM is installed, when `register_spikecore_target()` is called, then `relay.ext.spikecore` and `relay.ext.spikecore.cost_estimator` are registered in TVM's function registry
- [INFERRED] Given the cost estimator is registered, when queried, then it returns 1 (always prefer offloading to SpikeCore)
- [INFERRED] Given TVM is not installed, when `register_spikecore_target()` is called, then a `RuntimeError` is raised with message "TVM is required for BYOC target registration"

**Evidence:** FD-003, DA-018, DA-019, DA-020
**Code:** `spikecore/byoc_codegen.py` lines 95-114

---

#### Feature 1.4: Relay Graph Partitioning

Split a TVM Relay module into host subgraphs and SpikeCore-offloaded subgraphs.

##### US-1.4.1: Partition Relay Graph for SpikeCore Offload

**Description**
- As an ML Engineer with TVM installed
- I want to partition a Relay IR graph into host and SpikeCore subgraphs
- So that SpikeCore-compatible operations are offloaded to the neuromorphic target

**Acceptance Criteria**
- [INFERRED] Given a Relay module and parameters, when `partition_for_spikecore()` is called, then the three-pass pipeline executes: MergeComposite, AnnotateTarget, PartitionGraph
- [INFERRED] Given the partitioning pipeline, when executed, then it runs with `opt_level=3` pass context
- [DOCUMENTED] Given an MLP model, when partitioned, then `dense+relu` subgraphs are offloaded to SpikeCore and flatten/reshape stays on host
  - *DM-010: Design doc describes the partitioning architecture*

**Evidence:** FD-004, DA-018, DM-010
**Code:** `spikecore/byoc_codegen.py` lines 117-148

---

#### Feature 1.5: Relay Pattern Matching for SpikeCore

Define composite patterns that map TVM Relay subgraphs to SpikeCore primitives.

##### US-1.5.1: Retrieve SpikeCore Pattern Table

**Description**
- As an ML Engineer with TVM installed
- I want to retrieve the SpikeCore composite pattern table
- So that TVM's MergeComposite transform can fuse matching operator sequences

**Acceptance Criteria**
- [INFERRED] Given TVM is installed, when `spikecore_pattern_table()` is called, then it returns three named patterns: `spikecore.dense_relu`, `spikecore.dense_bias`, `spikecore.qnn_dense_clip`
- [DOCUMENTED] Given the pattern table, when used with MergeComposite, then `nn.dense + bias_add + relu` maps to ACC + FIRE, and `nn.dense + bias_add` maps to ACC only
  - *DM-008: Loihi architecture mapping table in README*

**Evidence:** FD-005, DA-013, DM-008
**Code:** `spikecore/relay_patterns.py` lines 65-78

---

##### US-1.5.2: BYOC Codegen Callback for Assembly Emission

**Description**
- As an ML Engineer with TVM installed
- I want the BYOC codegen callback to emit SpikeCore assembly from matched Relay subgraphs
- So that the compiled output contains runnable neuromorphic instructions

**Acceptance Criteria**
- [INFERRED] Given a Relay function containing `dense_relu` composite, when the codegen callback fires, then it emits ACC + FIRE + LEAK instructions for that subgraph
- [INFERRED] Given a Relay function containing `dense_bias` composite, when the codegen callback fires, then it emits ACC instructions only
- [INFERRED] Given any codegen callback result, when the output is inspected, then it ends with "HALT"

**Evidence:** FD-006, DA-018
**Code:** `spikecore/byoc_codegen.py` lines 54-92

---

### Epic 2: Quantize Neural Network Models for Integer-Only Hardware

**Goal:** An ML Engineer wants to quantize float32 model weights and activations to integer representations so that they are compatible with SpikeCore's integer-only datapath.

---

#### Feature 2.1: TVM Quantization Configuration

Apply int8 quantization through TVM's quantization pass infrastructure.

##### US-2.1.1: Quantize Relay Module for SpikeCore

**Description**
- As an ML Engineer with TVM installed
- I want to quantize a float32 Relay module to int8 using SpikeCore-tuned configuration
- So that the quantized model matches SpikeCore's hardware data types

**Acceptance Criteria**
- [INFERRED] Given a float32 Relay module, when `quantize_for_spikecore()` is called, then TVM's qconfig is applied with `nbit_input=8`, `nbit_weight=8`, `nbit_activation=16`, `dtype_input="int8"`, `dtype_weight="int8"`, `dtype_activation="int16"`
- [INFERRED] Given the quantization config, when calibration mode is set, then `calibrate_mode="global_scale"` and `global_scale=8.0` are used
- [INFERRED] Given TVM is not installed, when `quantize_for_spikecore()` is called, then a `RuntimeError` is raised

**Evidence:** FD-007, DA-021
**Code:** `spikecore/quantize.py` lines 22-53

---

#### Feature 2.2: Manual Symmetric Weight Quantization

Manually quantize float32 weights to int8 for the non-TVM path.

##### US-2.2.1: Quantize Weights Using Symmetric Quantization

**Description**
- As an ML Engineer
- I want to quantize float32 weight tensors to int8 using symmetric quantization
- So that weights fit in SpikeCore's 256-byte per-core weight memory

**Acceptance Criteria**
- [VERIFIED] Given a float32 weight tensor, when `manual_quantize_weights()` is called, then the output dtype is `int8` and `zero_point == 0` (symmetric)
  - *BA-019: test_symmetric_quantize verifies `q.dtype == np.int8` and `zp == 0`*
- [VERIFIED] Given a weight tensor with max absolute value 1.0, when quantized to 8 bits, then `scale = 1.0 / 127` and the value 1.0 maps to 127
  - *BA-019: asserts `abs(scale - 1.0/127) < 1e-5` and `q[0,0] == 127`*
- [VERIFIED] Given a weight tensor of all zeros, when quantized, then all quantized values are zero and scale defaults to 1.0
  - *BA-020: test_zero_weights verifies `np.all(q == 0)` and `scale == 1.0`*

**Evidence:** FD-008, DA-022, BA-019, BA-020
**Code:** `spikecore/quantize.py` lines 56-78

---

#### Feature 2.3: Manual Asymmetric Activation Quantization

Quantize activations (typically post-ReLU, non-negative) to int8 using asymmetric quantization.

##### US-2.3.1: Quantize Activations Using Asymmetric Quantization

**Description**
- As an ML Engineer
- I want to quantize activation tensors to int8 using asymmetric quantization
- So that ReLU outputs (range [0, max]) are efficiently represented

**Acceptance Criteria**
- [INFERRED] Given a non-negative activation tensor, when `manual_quantize_activations()` is called, then `qmax = 255` (unsigned 8-bit) and values are clipped to [0, 255]
- [INFERRED] Given an activation tensor with all values near zero (max < 1e-10), when quantized, then all quantized values are zero and scale defaults to 1.0

**Evidence:** FD-009, DA-023
**Code:** `spikecore/quantize.py` lines 81-104

---

### Epic 3: Simulate Neuromorphic Program Execution

**Goal:** An ML Engineer wants to execute compiled SpikeCore assembly on a virtual hardware model so that they can verify the correctness of the compilation pipeline and observe spike behavior.

---

#### Feature 3.1: SpikeCore CPU Instantiation and Configuration

Create and configure the virtual SpikeCore hardware model.

##### US-3.1.1: Instantiate SpikeCore CPU with Default Configuration

**Description**
- As an ML Engineer
- I want to create a SpikeCore CPU simulator with 128 neuron cores
- So that I have a virtual hardware target to execute compiled programs

**Acceptance Criteria**
- [VERIFIED] Given no arguments, when `SpikeCoreCPU()` is instantiated, then it has 128 cores
  - *BA-012: test verifies `SpikeCoreCPU()` has `num_cores == 128` implicitly via `NUM_CORES`*
- [VERIFIED] Given a newly created NeuronCore, when inspected, then its accumulator is 0, membrane potential is 0, and `has_spiked` is False
  - *BA-009: test_initial_state verifies all three fields*
- [VERIFIED] Given a NeuronCore with non-zero state, when `reset()` is called, then accumulator, membrane potential, and has_spiked are zeroed
  - *BA-010: test_reset verifies all three fields after reset*

**Evidence:** FD-010, DA-001, DA-002, BA-009, BA-010, BA-012
**Code:** `spikecore/hardware_model.py` lines 20-44

---

#### Feature 3.2: Weight Loading into Neuron Cores

Distribute quantized int8 weight matrices across neuron core local memory.

##### US-3.2.1: Load Weights into SpikeCore Cores

**Description**
- As an ML Engineer
- I want to load quantized int8 weight matrices into neuron core local weight memory
- So that each core has the weights it needs for accumulation

**Acceptance Criteria**
- [VERIFIED] Given a weight matrix of shape `(2, 4)`, when `load_weights()` is called, then core 0 holds row 0 and core 1 holds row 1 of the weights
  - *BA-011: test_load_weights verifies exact weight values per core*
- [INFERRED] Given a weight matrix requiring more cores than available, when `load_weights()` is called, then a `ValueError` is raised with a descriptive message
- [INFERRED] Given sequential `load_weights()` calls without explicit `core_offset`, when weights are loaded, then cumulative tracking automatically advances the core offset

**Evidence:** FD-011, DA-001, DA-002, BA-011
**Code:** `spikecore/hardware_model.py` lines 50-70

---

#### Feature 3.3: Program Loading and Execution

Load and execute an assembled program on the SpikeCore simulator.

##### US-3.3.1: Load Assembled Program

**Description**
- As an ML Engineer
- I want to load an assembled program (list of Instructions) into the SpikeCore CPU
- So that the simulator has a program to execute

**Acceptance Criteria**
- [VERIFIED] Given a list of Instruction objects, when `load_program()` is called, then the program is stored and accessible via `cpu.program`
  - *BA-012: test_load_program verifies `len(cpu.program) == 2`*

**Evidence:** FD-012, BA-012
**Code:** `spikecore/hardware_model.py` lines 46-48

---

##### US-3.3.2: Execute ACC Instruction (Weighted Accumulation)

**Description**
- As an ML Engineer
- I want the ACC instruction to perform a weighted accumulation from the activation bus
- So that each neuron core computes the dot product of input spikes and local weights

**Acceptance Criteria**
- [VERIFIED] Given core 0 with weights `[2, 3, 1, -1]` and input `[1, 1, 1, 1]`, when ACC executes, then the accumulator holds 5 (the dot product `2+3+1-1`)
  - *BA-013: test_acc_instruction verifies `cpu.cores[0].accumulator == 5`*
- [INFERRED] Given an ACC instruction with operands `(bank, start, end)`, when executed, then it reads activations from `bus[start:end]` and multiplies by `core.weights[:end-start]` using int32 arithmetic

**Evidence:** FD-013, DA-006, BA-013
**Code:** `spikecore/hardware_model.py` lines 72-84

---

##### US-3.3.3: Execute FIRE Instruction (Threshold Spike Emission)

**Description**
- As an ML Engineer
- I want the FIRE instruction to emit a spike when the accumulator exceeds a threshold
- So that neuron activation propagates to the next layer via the activation bus

**Acceptance Criteria**
- [VERIFIED] Given an accumulator value of 100 and FIRE threshold of 64, when FIRE executes, then a spike is emitted (`spike_counts[core_id] == 1`) and the spike is logged as `(timestep, core_id)`
  - *BA-014: test_fire_with_spike verifies `counts[0] == 1` and spike log entry `(0, 0)`*
- [VERIFIED] Given an accumulator value of 20 and FIRE threshold of 64, when FIRE executes, then no spike is emitted (`spike_counts[core_id] == 0`) and the spike log is empty
  - *BA-015: test_fire_no_spike verifies `counts[0] == 0` and `len(spike_log) == 0`*
- [INFERRED] Given FIRE fires with a shift operand > 0, when the activation is written to the bus, then the value is right-shifted by `shift` bits and clamped to [-128, 127] (int8 range)

**Evidence:** FD-013, DA-007, DA-008, DA-027, BA-014, BA-015
**Code:** `spikecore/hardware_model.py` lines 86-108

---

##### US-3.3.4: Execute LEAK Instruction (Membrane Decay)

**Description**
- As an ML Engineer
- I want the LEAK instruction to decay the membrane potential using a fixed-point multiply
- So that neurons exhibit temporal dynamics with leaky integrate-and-fire behavior

**Acceptance Criteria**
- [VERIFIED] Given membrane potential 256 and decay factor 128, when LEAK executes, then membrane potential becomes 128 (256 * 128 / 256 = 128, via right-shift by 8)
  - *BA-016: test_leak_decays_membrane verifies `cpu.cores[0].membrane_potential == 128`*
- [VERIFIED] Given an ACC -> FIRE (miss) -> LEAK sequence with accumulation 60, threshold 100, and decay 128, when executed, then membrane potential is 30 (60 * 128 / 256)
  - *BA-016: test_leak_in_program verifies full integration `membrane_potential == 30`*

**Evidence:** FD-013, DA-009, BA-016
**Code:** `spikecore/hardware_model.py` lines 110-115

---

##### US-3.3.5: Execute HALT Instruction (Program Termination)

**Description**
- As an ML Engineer
- I want the HALT instruction to stop program execution
- So that the simulator terminates after the program is complete

**Acceptance Criteria**
- [VERIFIED] Given a program containing HALT, when execution reaches HALT, then the simulator breaks out of the instruction loop
  - *BA-018: test_fire_with_spike and others verify HALT stops execution implicitly*
- [VERIFIED] Given a compiled program, when the last instruction is inspected, then it is always HALT
  - *BA-024: test_single_layer_with_relu and test_compilation_produces_valid_assembly both verify HALT termination*

**Evidence:** FD-013, DA-003, DA-004, BA-018, BA-024
**Code:** `spikecore/hardware_model.py` line 168

---

##### US-3.3.6: Execute Multi-Neuron Integration Test

**Description**
- As an ML Engineer
- I want to simulate a multi-neuron network with ACC, FIRE, and LEAK interactions
- So that I can verify correct inter-neuron spike propagation

**Acceptance Criteria**
- [VERIFIED] Given a 2-neuron network where core 0 has weights `[20,20,20]` (sum 60 >= threshold 30) and core 1 has weights `[10,10,5]` (sum 25 < threshold 30), when input `[1,1,1]` is applied, then core 0 fires and core 1 does not
  - *BA-017: test_two_neuron_network verifies `counts[0] == 1` and `counts[1] == 0`*

**Evidence:** FD-013, DA-025, BA-017
**Code:** `spikecore/hardware_model.py` lines 117-176 (run method)

---

#### Feature 3.4: Run Simulation with Timesteps

Execute the loaded program for a configurable number of timesteps with event-driven semantics.

##### US-3.4.1: Run Simulation and Collect Results

**Description**
- As an ML Engineer
- I want to run a simulation for N timesteps and collect spike counts and logs
- So that I can analyze temporal spiking behavior and read output activations

**Acceptance Criteria**
- [VERIFIED] Given a loaded program and input spikes, when `run()` is called with `timesteps=1`, then spike counts are returned as an array and `get_spike_log()` returns `(timestep, core_id)` tuples
  - *BA-014: verifies spike count and log for 1 timestep*
- [INFERRED] Given multi-timestep execution, when each timestep begins, then input spikes are re-injected and the bus is cleared except for inputs
- [INFERRED] Given a program with layer-spanning ACC instructions, when ACC reads from the bus, then same-layer cores read from a consistent snapshot (parallel execution semantics)

**Evidence:** FD-013, DA-005, DA-010, DA-025, DA-026, BA-014
**Code:** `spikecore/hardware_model.py` lines 117-177

---

#### Feature 3.5: Read Output Activations

Read final accumulator plus membrane potential values from designated output cores.

##### US-3.5.1: Get Output Activations from Output Cores

**Description**
- As an ML Engineer
- I want to read the combined accumulator and membrane potential values from output layer cores
- So that I can obtain classification logits from the simulated neuromorphic inference

**Acceptance Criteria**
- [VERIFIED] Given cores 0 and 1 with weights `[30,40]` and `[10,20]` respectively, and input `[1,1]`, when `get_output_activations([0, 1])` is called after running ACC-only instructions, then outputs are `[70, 30]`
  - *BA-013: test_get_output_activations verifies exact accumulator readout*

**Evidence:** FD-014, DA-026, BA-013
**Code:** `spikecore/hardware_model.py` lines 183-193

---

### Epic 4: Work with SpikeCore Assembly Language

**Goal:** An ML Engineer wants to read, write, and transform SpikeCore assembly text so that they can inspect, modify, and roundtrip neuromorphic programs.

---

#### Feature 4.1: Assemble Text to Instructions

Parse SpikeCore assembly text into structured Instruction objects.

##### US-4.1.1: Parse ACC Instruction from Text

**Description**
- As an ML Engineer
- I want to parse an ACC instruction from assembly text
- So that I can load hand-written or edited assembly programs

**Acceptance Criteria**
- [VERIFIED] Given text `"ACC core_3 weight_bank_0 spike_in_[0:8]"`, when `assemble()` is called, then it returns one Instruction with `opcode=ACC`, `core_id=3`, `operands=(0, 0, 8)`
  - *BA-001: test_assemble_acc verifies all three fields exactly*

**Evidence:** FD-016, DA-011, BA-001
**Code:** `spikecore/assembly.py` lines 97-101

---

##### US-4.1.2: Parse FIRE Instruction from Text

**Description**
- As an ML Engineer
- I want to parse a FIRE instruction (with optional shift operand) from assembly text
- So that I can hand-author or modify threshold/shift parameters

**Acceptance Criteria**
- [VERIFIED] Given text `"FIRE core_5 threshold_64"`, when assembled, then it returns one Instruction with `opcode=FIRE`, `core_id=5`, `operands=(64,)`
  - *BA-002: test_assemble_fire verifies all fields*
- [VERIFIED] Given text with shift operand `"FIRE core_5 threshold_64 shift_7"`, when assembled, then `operands=(64, 7)`
  - *BA-003: test_assemble_fire_with_shift (implied by regex pattern support)*

**Evidence:** FD-016, DA-011, BA-002, BA-003
**Code:** `spikecore/assembly.py` lines 103-108

---

##### US-4.1.3: Parse LEAK Instruction from Text

**Description**
- As an ML Engineer
- I want to parse a LEAK instruction from assembly text
- So that I can inspect or modify membrane decay parameters

**Acceptance Criteria**
- [VERIFIED] Given text `"LEAK core_0 decay_240"`, when assembled, then it returns one Instruction with `opcode=LEAK`, `core_id=0`, `operands=(240,)`
  - *BA-004: test_assemble_leak verifies all fields*

**Evidence:** FD-016, DA-011, BA-004
**Code:** `spikecore/assembly.py` lines 110-113

---

##### US-4.1.4: Parse NOP and HALT Instructions

**Description**
- As an ML Engineer
- I want to parse NOP and HALT control instructions from assembly text
- So that I can include pipeline bubbles and program termination in hand-written assembly

**Acceptance Criteria**
- [VERIFIED] Given text `"NOP\nHALT"`, when assembled, then two instructions are returned with opcodes NOP and HALT respectively
  - *BA-005: test_assemble_nop_halt verifies both opcodes and count*

**Evidence:** FD-016, DA-011, BA-005
**Code:** `spikecore/assembly.py` lines 92-96

---

##### US-4.1.5: Handle Comments and Blank Lines in Assembly

**Description**
- As an ML Engineer
- I want the assembler to ignore comments (lines starting with `#` or `//`) and blank lines
- So that I can annotate assembly listings without affecting parsing

**Acceptance Criteria**
- [VERIFIED] Given assembly text with `#` comments, `//` comments, and blank lines interspersed with valid instructions, when assembled, then only valid instructions are parsed
  - *BA-006: test_assemble_ignores_comments verifies `len(prog) == 1` with surrounding comments*

**Evidence:** FD-016, DA-011, BA-006
**Code:** `spikecore/assembly.py` lines 85-86

---

##### US-4.1.6: Handle Address-Prefixed Assembly Lines

**Description**
- As an ML Engineer
- I want the assembler to handle lines with address prefixes (e.g., `"0000: ACC..."`)
- So that disassembled output can be re-assembled without modification

**Acceptance Criteria**
- [VERIFIED] Given assembly text with `"0000: ACC core_0 ..."` and `"0001: HALT"`, when assembled, then address prefixes are stripped and two valid instructions are returned
  - *BA-006: test_assemble_with_address_prefix verifies `len(prog) == 2`*

**Evidence:** FD-016, DA-011, BA-006
**Code:** `spikecore/assembly.py` lines 88-90

---

##### US-4.1.7: Report Invalid Instructions with Context

**Description**
- As an ML Engineer
- I want the assembler to raise a clear error with line number context for unparseable instructions
- So that I can quickly locate and fix syntax errors in assembly files

**Acceptance Criteria**
- [VERIFIED] Given text `"BADOP core_0"`, when assembled, then a `ValueError` is raised with `"Cannot parse"` in the message
  - *BA-008: test_bad_instruction_raises verifies exact error pattern*
- [INFERRED] Given a multi-line assembly with error on line N, when assembled, then the error message includes `"Line N:"`

**Evidence:** FD-016, DA-011, BA-008
**Code:** `spikecore/assembly.py` lines 115-128

---

#### Feature 4.2: Disassemble Instructions to Text

Convert structured Instruction objects back to human-readable assembly text.

##### US-4.2.1: Roundtrip Disassemble and Reassemble

**Description**
- As an ML Engineer
- I want to disassemble a program to text and reassemble it, yielding identical instructions
- So that I can trust the text representation for inspection, editing, and storage

**Acceptance Criteria**
- [VERIFIED] Given a program with ACC, FIRE, LEAK, and HALT instructions, when disassembled to text and then reassembled, then every instruction's opcode, core_id, and operands match the originals
  - *BA-007: test_roundtrip_disassemble_assemble verifies field-by-field equality*

**Evidence:** FD-017, DA-012, BA-007
**Code:** `spikecore/assembly.py` lines 43-71

---

### Epic 5: Verify Compilation Pipeline Correctness

**Goal:** An ML Engineer wants to verify end-to-end that the SpikeCore compilation pipeline produces correct inference results so that they can trust the pipeline for real models.

---

#### Feature 5.1: Round-Trip Accuracy Verification

Verify that SpikeCore's quantized integer inference matches PyTorch's float inference.

##### US-5.1.1: Verify Float Model Baseline Accuracy

**Description**
- As an ML Engineer
- I want to confirm that the trained float32 MLP achieves high accuracy on training data
- So that I have a reliable baseline for comparison with the quantized SpikeCore path

**Acceptance Criteria**
- [VERIFIED] Given a numpy MLP trained on synthetic data for 200 epochs, when evaluated on training data, then accuracy exceeds 90%
  - *BA-025: test_pytorch_accuracy asserts `accuracy > 0.90`*

**Evidence:** FD-025, BA-025
**Code:** `tests/test_roundtrip.py` lines 136-142

---

##### US-5.1.2: Verify SpikeCore Matches PyTorch Predictions

**Description**
- As an ML Engineer
- I want the SpikeCore quantized inference to match PyTorch's top-1 predictions on at least 70% of test samples
- So that I can confirm the compilation pipeline preserves model semantics despite quantization

**Acceptance Criteria**
- [VERIFIED] Given a trained MLP compiled to SpikeCore assembly, when comparing top-1 predictions on 100 samples, then the match rate is >= 70%
  - *BA-026: test_spikecore_matches_pytorch asserts `match_rate >= 0.70`*
- [DOCUMENTED] Given the verification criteria in the design document, when the round-trip test passes, then the system meets the "Round-trip accuracy" verification criterion
  - *DM-024: Design doc targets >= 70% match on synthetic data, >= 90% on MNIST*

**Evidence:** FD-025, FD-026, DA-024, BA-026, DM-024
**Code:** `tests/test_roundtrip.py` lines 144-179

---

##### US-5.1.3: Verify Compiled Assembly Structure

**Description**
- As an ML Engineer
- I want the compiled program to have a valid instruction structure
- So that I can confirm the codegen produces well-formed assembly

**Acceptance Criteria**
- [VERIFIED] Given a compiled two-layer MLP, when the program is inspected, then it ends with HALT and the number of ACC instructions equals the total output neurons across all layers
  - *BA-027: test_compilation_produces_valid_assembly verifies HALT and ACC count*

**Evidence:** FD-026, BA-027
**Code:** `tests/test_roundtrip.py` lines 181-195

---

##### US-5.1.4: Verify Spike Activity During Simulation

**Description**
- As an ML Engineer
- I want to confirm that the simulator produces non-empty spike activity
- So that I know the event-driven execution is functioning

**Acceptance Criteria**
- [VERIFIED] Given a compiled program with ReLU hidden layers and non-trivial input, when simulated for 1 timestep, then the spike log is non-empty
  - *BA-028: test_spike_log_nonempty asserts `len(spike_log) > 0`*

**Evidence:** FD-026, BA-028
**Code:** `tests/test_roundtrip.py` lines 197-220

---

### Epic 6: Visualize Neuromorphic Compilation and Simulation

**Goal:** An ML Engineer wants to visualize the compilation pipeline, simulation results, and model parameters so that they can gain insight into neuromorphic behavior and verify correctness visually.

---

#### Feature 6.1: Spike Raster Plot

Visualize neuron core spiking activity across timesteps.

##### US-6.1.1: Plot Spike Raster from Spike Log

**Description**
- As an ML Engineer
- I want to visualize which neuron cores fire at each timestep as a scatter plot
- So that I can observe temporal spiking patterns and verify activity distribution

**Acceptance Criteria**
- [INFERRED] Given a spike log of `(timestep, core_id)` tuples, when `plot_spike_raster()` is called, then a matplotlib Figure is returned with neurons on y-axis and timesteps on x-axis
- [INFERRED] Given an empty spike log, when `plot_spike_raster()` is called, then a figure is returned with a "No spikes recorded" message centered in the axes

**Evidence:** FD-018, DA-028
**Code:** `spikecore/visualize.py` lines 17-67

---

#### Feature 6.2: Compilation Graph Visualization

Visualize the partitioned compilation graph showing host vs. SpikeCore layer targets.

##### US-6.2.1: Plot Partitioned Compilation Graph

**Description**
- As an ML Engineer
- I want to see a visual diagram of how the model's layers are partitioned between host CPU and SpikeCore
- So that I can understand which operations are offloaded to the neuromorphic target

**Acceptance Criteria**
- [INFERRED] Given lists of layer names and targets, when `plot_compilation_graph()` is called, then layers are drawn as colored boxes (blue for SpikeCore, gray for host) connected by arrows
- [INFERRED] Given optional `layer_shapes`, when provided, then each box displays the shape annotation (e.g., "784 -> 64")

**Evidence:** FD-019, DA-029
**Code:** `spikecore/visualize.py` lines 70-134

---

#### Feature 6.3: Weight Distribution Histogram

Visualize the distribution of quantized int8 weight values.

##### US-6.3.1: Plot Quantized Weight Histogram

**Description**
- As an ML Engineer
- I want to see a histogram of quantized int8 weight values per layer
- So that I can assess quantization quality and detect distribution anomalies

**Acceptance Criteria**
- [INFERRED] Given an int8 weight array, when `plot_weight_distribution()` is called, then a histogram with bins [-128, 127] is plotted with a vertical line at zero and stats overlay (mean, std, range)

**Evidence:** FD-020, DA-030
**Code:** `spikecore/visualize.py` lines 137-172

---

#### Feature 6.4: PyTorch vs. SpikeCore Output Comparison

Side-by-side bar chart comparing PyTorch softmax probabilities and SpikeCore raw scores.

##### US-6.4.1: Plot Comparison of PyTorch and SpikeCore Outputs

**Description**
- As an ML Engineer
- I want to see a side-by-side comparison of PyTorch probabilities and SpikeCore scores for the same input
- So that I can visually verify prediction agreement and quantization impact

**Acceptance Criteria**
- [INFERRED] Given PyTorch probabilities and SpikeCore scores arrays, when `plot_comparison()` is called, then two bar charts are plotted side-by-side with the predicted class highlighted in red
- [INFERRED] Given matching or mismatching predictions, when the figure title is rendered, then it displays "[MATCH]" (green) or "[MISMATCH]" (red)

**Evidence:** FD-021, DA-031
**Code:** `spikecore/visualize.py` lines 175-228

---

### Epic 7: Explore the Pipeline via Guided Notebook

**Goal:** An ML Engineer wants to walk through the entire neuromorphic compilation pipeline in an interactive Jupyter notebook so that they can learn each stage and inspect intermediate results.

---

#### Feature 7.1: End-to-End Notebook Walkthrough

An interactive 10-cell Jupyter notebook guiding the user through every pipeline stage.

##### US-7.1.1: Setup and Import Verification

**Description**
- As an ML Engineer
- I want the notebook to import all required libraries and verify TVM availability
- So that I know my environment is correctly configured before starting

**Acceptance Criteria**
- [DOCUMENTED] Given the notebook Cell 1, when executed, then it imports numpy, matplotlib, torch, and all spikecore modules, prints TVM availability status, and confirms "All imports successful"
  - *DM-009: README identifies this as an "educational platform"*

**Evidence:** FD-022, DM-009
**Code:** `notebooks/01_spikecore_tvm.ipynb` Cell 2

---

##### US-7.1.2: Train and Evaluate PyTorch Model

**Description**
- As an ML Engineer
- I want the notebook to train a 2-layer MLP on MNIST and report accuracy
- So that I have a baseline model for the compilation pipeline

**Acceptance Criteria**
- [DOCUMENTED] Given Cell 2 (Model), when executed, then a MNISTNet (784 -> 64 -> 10) is trained for 2 epochs on MNIST and test accuracy is printed
  - *DM-004: MNIST MLP identified as the minimal model choice*

**Evidence:** FD-022, DA-032, DM-004
**Code:** `notebooks/01_spikecore_tvm.ipynb` Cell 4

---

##### US-7.1.3: Export Model and Quantize Weights

**Description**
- As an ML Engineer
- I want the notebook to extract PyTorch weights and quantize them to int8
- So that I can see the quantization process and its effect on weight values

**Acceptance Criteria**
- [DOCUMENTED] Given Cells 3 and 4 (Export and Quantize), when executed, then weights are extracted as numpy arrays, quantized using symmetric quantization, and quantization error statistics are printed

**Evidence:** FD-022, DA-022, DM-006
**Code:** `notebooks/01_spikecore_tvm.ipynb` Cells 6, 8

---

##### US-7.1.4: Compile, Simulate, and Compare

**Description**
- As an ML Engineer
- I want the notebook to compile to SpikeCore assembly, run the simulator, and compare predictions
- So that I can see the full pipeline from model to neuromorphic inference

**Acceptance Criteria**
- [DOCUMENTED] Given Cells 7, 8, and 9 (Codegen, Simulate, Compare), when executed, then the assembly listing is printed, simulation produces spike counts, and a comparison chart shows PyTorch vs. SpikeCore agreement percentage
- [DOCUMENTED] Given the full notebook execution, when completed, then a pipeline summary table is printed mapping each stage to its real-world neuromorphic equivalent
  - *DM-008: Loihi architecture mapping table*

**Evidence:** FD-022, DA-025, DM-008, DM-009
**Code:** `notebooks/01_spikecore_tvm.ipynb` Cells 14, 16, 18, 20

---

### Epic 8: Set Up Development Environment

**Goal:** An ML Engineer wants to set up a reproducible development environment so that they can run the complete pipeline including TVM compilation.

---

#### Feature 8.1: Docker Build with TVM from Source

Build a Docker image containing TVM compiled from source, PyTorch CPU, and Jupyter.

##### US-8.1.1: Build TVM Docker Image

**Description**
- As an ML Engineer
- I want to build a Docker image that compiles TVM from source with LLVM support
- So that I have a reproducible environment with TVM BYOC capabilities

**Acceptance Criteria**
- [DOCUMENTED] Given the Dockerfile, when `docker compose build` is executed, then a multi-stage build compiles TVM from source with LLVM-15 and produces a runtime image with TVM Python bindings
  - *DM-002: Docker chosen for TVM reproducibility*
- [INFERRED] Given the build stage, when complete, then TVM is built with `USE_LLVM` and `USE_RELAY_DEBUG=ON` enabled

**Evidence:** FD-023, DM-002
**Code:** `Dockerfile` lines 1-63

---

#### Feature 8.2: Docker Compose Launch

Launch the development environment with a single command.

##### US-8.2.1: Launch Lab with Docker Compose

**Description**
- As an ML Engineer
- I want to launch the full environment with `docker compose up`
- So that I can access Jupyter notebook on localhost:8888

**Acceptance Criteria**
- [DOCUMENTED] Given the docker-compose.yml, when `docker compose up` is executed, then Jupyter starts on port 8888 with token "spikecore"
  - *DM-009: README quick-start documents this exact workflow*
- [INFERRED] Given the compose configuration, when the container runs, then `./notebooks`, `./spikecore`, and `./tests` are volume-mounted for live editing

**Evidence:** FD-024, DM-009
**Code:** `docker-compose.yml` lines 1-10

---

### Epic 9: Define SpikeCore Instruction Set Architecture

**Goal:** An ML Engineer wants a well-defined instruction set architecture so that compiled programs are unambiguous and the assembler/disassembler can operate correctly.

---

#### Feature 9.1: SpikeCore ISA Definition

Define the 5-opcode instruction set: ACC, FIRE, LEAK, NOP, HALT.

##### US-9.1.1: Define Opcode Enumeration

**Description**
- As an ML Engineer
- I want a well-defined enumeration of SpikeCore opcodes with integer values
- So that instructions can be encoded, decoded, and pattern-matched reliably

**Acceptance Criteria**
- [INFERRED] Given the Opcode enum, when inspected, then it contains exactly 5 values: ACC=0, FIRE=1, LEAK=2, NOP=3, HALT=4
- [VERIFIED] Given any valid assembly text, when parsed, then each instruction maps to exactly one Opcode enum value
  - *BA-001 through BA-005: each test verifies correct opcode assignment*

**Evidence:** FD-015, DA-003, DA-004, BA-001, BA-002, BA-003, BA-004, BA-005
**Code:** `spikecore/assembly.py` lines 23-28

---

##### US-9.1.2: Define Instruction Data Structure

**Description**
- As an ML Engineer
- I want instructions represented as frozen dataclasses with opcode, core_id, and operands
- So that programs are immutable, hashable, and safely shareable

**Acceptance Criteria**
- [INFERRED] Given the Instruction dataclass, when inspected, then it is frozen (immutable) with fields: `opcode: Opcode`, `core_id: int = 0`, `operands: tuple[int, ...] = ()`
- [INFERRED] Given an Instruction object, when `repr()` is called, then it returns the disassembled text representation

**Evidence:** FD-015, DA-003
**Code:** `spikecore/assembly.py` lines 31-41

---

---

## 3. Entity Glossary

| Term | Definition | Source |
|------|-----------|--------|
| **NeuronCore** | A single neuron processing unit with local state: accumulator (int32), membrane potential, weight memory (256 bytes of int8), and spike status. One core per output neuron. | DA-001 |
| **SpikeCoreCPU** | The top-level simulator containing 128 NeuronCores, a program counter, activation bus, and spike log. Executes programs in event-driven timestep fashion. | DA-002 |
| **Instruction** | A frozen dataclass representing one SpikeCore machine instruction with opcode, core_id, and operands tuple. | DA-003 |
| **Opcode** | IntEnum with 5 values: ACC (0), FIRE (1), LEAK (2), NOP (3), HALT (4). | DA-004 |
| **Activation Bus** | A shared int16 array for inter-layer communication. Input spikes are loaded into the bus; FIRE instructions write spike outputs back to the bus for downstream layers. | DA-005 |
| **ACC (Weighted Accumulate)** | Computes dot product of bus slice `[start:end]` with core-local weights and adds the result to the core's accumulator. Uses int32 arithmetic. | DA-006 |
| **FIRE (Threshold + Spike)** | Compares `accumulator + membrane_potential` against threshold. If exceeded: right-shifts by scale factor, clamps to int8 [-128, 127], writes to bus, resets state. If not: transfers accumulator to membrane potential. | DA-007, DA-008, DA-027 |
| **LEAK (Membrane Decay)** | Decays membrane potential by `(membrane_potential * decay) >> 8`, implementing a fixed-point multiply by `decay/256`. | DA-009 |
| **Layer-Aware Snapshotting** | Within a layer (detected by same ACC input range), all cores read from a frozen snapshot of the bus, simulating parallel execution. Snapshot refreshes at layer boundaries. | DA-010 |
| **Assembly Text Format** | Human-readable format: `ACC core_N weight_bank_B spike_in_[S:E]`, `FIRE core_N threshold_T [shift_S]`, `LEAK core_N decay_D`, `NOP`, `HALT`. Address prefixes `NNNN:` and comments `#`/`//` are supported. | DA-011 |
| **Round-Trip Fidelity** | The property that `assemble(disassemble(program))` yields instructions identical to the original program. | DA-012 |
| **Relay Pattern** | A TVM dataflow pattern matching a subgraph of Relay operators. Three patterns: dense_relu, dense_bias, qnn_dense_clip. | DA-013 |
| **LayerConfig** | A dict with keys `name`, `in_features`, `out_features`, `activation` used to describe each neural network layer for the standalone compiler. | DA-014 |
| **Core Allocation (Sequential)** | Cores are allocated sequentially across layers: layer 0 occupies cores `[0, out_features_0)`, layer 1 occupies cores `[out_features_0, out_features_0 + out_features_1)`, etc. | DA-015 |
| **FIRE Shift** | Right-shift applied during FIRE to rescale wide accumulators back to int8. Computed as `ceil(log2(fan_in)) + 7`. | DA-017 |
| **BYOC 3-Pass Pipeline** | TVM's MergeComposite -> AnnotateTarget -> PartitionGraph sequence that splits a Relay graph into host and SpikeCore subgraphs. | DA-018 |
| **Dual Execution Path** | The system supports two compilation paths: TVM BYOC (full pipeline, requires TVM) and standalone (manual quantization, no TVM). Both produce equivalent SpikeCore assembly. | DA-019 |
| **Symmetric Quantization** | Weight quantization with `zero_point=0`, `scale = max(|w|) / 127`. Maps float weights to int8 range [-127, 127] symmetrically around zero. | DA-022 |
| **Asymmetric Quantization** | Activation quantization for non-negative values (post-ReLU). Uses `qmax=255`, maps `[0, max]` to `[0, 255]`. | DA-023 |
| **MNISTNet** | The reference model: a 2-layer MLP with architecture 784 -> 64 (ReLU) -> 10 for MNIST digit classification. | DA-032 |

---

## 4. User Roles

### ML Engineer / Researcher

**Description:** A single-role system. The user is an AI/ML Engineer or Researcher exploring neuromorphic compilation, hardware simulation, and the TVM BYOC framework. They interact with the system through:

- **Python API calls** to compilation, quantization, assembly, and simulation functions
- **Jupyter notebook** cells that walk through the end-to-end pipeline
- **CLI commands** for Docker build/launch and pytest execution

**Capabilities:**
- Define and train PyTorch neural network models
- Compile models to SpikeCore neuromorphic assembly via TVM BYOC or standalone path
- Quantize float32 weights and activations to int8
- Load programs and weights into the SpikeCore simulator
- Run event-driven simulations for configurable timesteps
- Read assembly listings, spike logs, and output activations
- Visualize compilation graphs, spike rasters, weight distributions, and prediction comparisons
- Run automated test suites to verify pipeline correctness

**Authentication:** None. No multi-role, authorization, or access control mechanisms exist.

---

## 5. Non-Functional Requirements

### 5.1 Security

| ID | Requirement | Severity | Evidence |
|----|------------|----------|---------|
| NFR-SEC-001 | Jupyter notebook token is hardcoded as `"spikecore"` in Dockerfile CMD | Medium | IA-001, `Dockerfile` line 63 |
| NFR-SEC-002 | Docker container runs as root (no `USER` directive) | Medium | IA-002, `Dockerfile` |
| NFR-SEC-003 | Jupyter binds to `0.0.0.0` (all interfaces) inside container | Low | IA-003, `Dockerfile` line 61 |
| NFR-SEC-004 | No secret management infrastructure (no `.env` files, no vault integration) | Low | IA-004 |
| NFR-SEC-005 | Input validation is basic: assembly parser rejects malformed lines, weight loader validates core bounds, but no comprehensive sanitization | Low | IA-005, `assembly.py` line 115, `hardware_model.py` line 65 |

### 5.2 Performance

| ID | Requirement | Evidence |
|----|------------|---------|
| NFR-PERF-001 | Hardware model supports 128 neuron cores with 256 bytes of weight memory per core | IA-006, DA-001, DA-002 |
| NFR-PERF-002 | Integer-only datapath: int8 weights, int32 accumulator arithmetic (no floating point in simulation) | IA-007, DA-006 |
| NFR-PERF-003 | FIRE instruction uses right-shift for rescaling (single-cycle equivalent on hardware) | IA-008, DA-017 |
| NFR-PERF-004 | LEAK uses fixed-point multiply via right-shift by 8 (avoids division) | IA-009, DA-009 |
| NFR-PERF-005 | TVM compilation uses `opt_level=3` pass context for maximum optimization | IA-010, `byoc_codegen.py` line 145 |
| NFR-PERF-006 | PyTorch runs CPU-only (no CUDA/GPU dependency) | IA-011, DM-005, `requirements.txt` |
| NFR-PERF-007 | Quantization reduces model size by ~4x (float32 to int8) | IA-012, DA-022 |

### 5.3 Reliability

| ID | Requirement | Evidence |
|----|------------|---------|
| NFR-REL-001 | Dual execution path fallback: if TVM is unavailable, standalone path provides equivalent compilation | IA-013, DA-019 |
| NFR-REL-002 | Weight loading validates core bounds and raises `ValueError` if cores are exhausted | IA-014, `hardware_model.py` line 65 |
| NFR-REL-003 | ACC range clamping: `actual_end = min(end, len(bus))` prevents bus overflow | IA-015, `hardware_model.py` line 77 |
| NFR-REL-004 | Layer-aware bus snapshotting ensures parallel execution semantics (same-layer cores read consistent state) | IA-016, DA-010 |
| NFR-REL-005 | Zero-division protection in quantization: weights near zero (max < 1e-10) return scale=1.0 | IA-017, `quantize.py` lines 73-74 |
| NFR-REL-006 | Assembly parse errors include line number context for debugging | IA-018, `assembly.py` line 127 |

### 5.4 Observability

| ID | Requirement | Evidence |
|----|------------|---------|
| NFR-OBS-001 | Spike logging: every spike is recorded as `(timestep, core_id)` tuple in `spike_log` | IA-019, `hardware_model.py` line 173 |
| NFR-OBS-002 | Spike counts per core returned as numpy array from `run()` | IA-020, `hardware_model.py` line 177 |
| NFR-OBS-003 | Output activation readout via `get_output_activations()` for designated cores | IA-021, `hardware_model.py` lines 183-193 |
| NFR-OBS-004 | Four visualization functions: spike raster, compilation graph, weight histogram, comparison plot | IA-022, FD-018 through FD-021 |
| NFR-OBS-005 | No structured logging framework (no `logging` module, no log levels, print-only) | IA-023 |

### 5.5 Deployment

| ID | Requirement | Evidence |
|----|------------|---------|
| NFR-DEP-001 | Multi-stage Docker build: TVM compiled from source in builder stage, runtime stage is `python:3.11-slim` | IA-024, `Dockerfile` |
| NFR-DEP-002 | TVM environment variables set: `TVM_HOME`, `PYTHONPATH`, `TVM_LIBRARY_PATH` | IA-025, `Dockerfile` lines 47-49 |
| NFR-DEP-003 | Volume mounts for live editing: `./notebooks`, `./spikecore`, `./tests` | IA-026, `docker-compose.yml` lines 6-9 |
| NFR-DEP-004 | Jupyter exposed on port 8888 | IA-027, `docker-compose.yml` line 5 |
| NFR-DEP-005 | Python dependencies pinned in `requirements.txt` (torch 2.2.2+cpu, numpy 1.26.4, etc.) | IA-028, `requirements.txt` |
| NFR-DEP-006 | TVM built with `USE_RELAY_DEBUG=ON` for development/debugging | IA-029, `Dockerfile` line 29 |
| NFR-DEP-007 | pytest 8.3.3 included for running test suite | IA-030, `requirements.txt` line 8 |
| NFR-DEP-008 | `PYTHONDONTWRITEBYTECODE=1` set in compose environment | IA-031, `docker-compose.yml` line 11 |
| NFR-DEP-009 | No CI/CD pipeline configuration present | IA-032 |
| NFR-DEP-010 | No production deployment artifacts (no Kubernetes manifests, no health checks) | IA-032 |

---

## 6. Workflow Maps

### WA-001: Full Compilation Pipeline (End-to-End)

```
[Train PyTorch Model]
        |
        v
[Extract Weights as numpy]
        |
        v
[Check TVM Availability] ---> TVM available? --Yes--> [WA-002: TVM BYOC Path]
        |                                                      |
        No                                                     |
        |                                                      |
        v                                                      v
[WA-003: Standalone Path]                            [Partitioned Relay Module]
        |                                                      |
        v                                                      v
[SpikeCore Assembly (list[Instruction])]            [SpikeCore Assembly Text]
        |                                                      |
        v                                                      v
[WA-004: SpikeCore Program Execution]
        |
        v
[Spike Counts + Spike Log + Output Activations]
        |
        v
[Visualization (raster, graph, histogram, comparison)]
```

**Entry:** User has a trained PyTorch model
**Exit:** Visualized comparison of PyTorch vs. SpikeCore predictions
**Happy Path:** Model trains -> weights extracted -> compiled -> simulated -> results match
**Error Path:** TVM import failure triggers graceful fallback to standalone path

---

### WA-002: TVM BYOC Compilation Path

```
[Relay Module + Params]
        |
        v
[register_spikecore_target()] -- registers relay.ext.spikecore + cost_estimator
        |
        v
[quantize_for_spikecore(mod, params)] -- float32 -> int8 via TVM qconfig
        |
        v
[partition_for_spikecore(mod, params)]
        |
        +--> [MergeComposite(patterns)] -- fuse dense+relu, dense+bias, qnn+clip
        |
        +--> [AnnotateTarget(["spikecore"])] -- mark fused regions
        |
        +--> [PartitionGraph()] -- split host vs. spikecore subgraphs
        |
        v
[Partitioned IRModule]
        |
        v
[BYOC codegen callback] -- traverse subgraphs -> emit assembly text
        |
        v
[SpikeCore Assembly Text]
```

**Requires:** TVM installed (Docker environment)
**State Machine (SM-003):** HAS_TVM flag checked at each TVM-dependent entry point

---

### WA-003: Standalone Compilation Path (No TVM)

```
[PyTorch state_dict (numpy)] + [layer_configs]
        |
        v
[compile_from_torch(state_dict, layer_configs)]
        |
        +--> For each layer:
        |        |
        |        +--> [manual_quantize_weights(w_fp32)] -- symmetric, scale=max(|w|)/127
        |        |
        |        +--> [_compute_fire_shift(fan_in)] -- ceil(log2(fan_in)) + 7
        |        |
        |        +--> Generate instructions per output neuron:
        |                 - ACC core_N weight_bank_0 spike_in_[0:in_feat]
        |                 - FIRE core_N threshold_0 shift_S  (if relu)
        |                 - LEAK core_N decay_240             (if relu)
        |
        +--> Append HALT
        |
        v
[(program: list[Instruction], quantized_weights: list[(int8, scale)])]
```

**Requires:** numpy only (no TVM, no Docker)
**Core Allocation:** Sequential across layers; tracked by `core_offset`

---

### WA-004: SpikeCore Program Execution Lifecycle

```
State Machine (SM-001: NeuronCore Lifecycle):

  [IDLE] --load_weights()--> [WEIGHTS_LOADED]
           --load_program()--> [READY]
                  --run()--> [EXECUTING]
                      |
                      +--> For each timestep:
                      |        |
                      |        +--> Reset all cores
                      |        +--> Inject input spikes into bus
                      |        +--> For each instruction:
                      |        |        ACC  -> weighted accumulate from snapshot
                      |        |        FIRE -> threshold check, spike/no-spike
                      |        |        LEAK -> membrane decay
                      |        |        HALT -> break
                      |        |
                      |        +--> Collect spikes, update counts
                      |
                      +--> [COMPLETE]
                              |
                              v
                      [Read outputs via get_output_activations()]
                      [Read spike log via get_spike_log()]
```

**State Machine (SM-002: Program Execution State):**
- IDLE -> LOADED (load_program) -> RUNNING (run) -> HALTED (HALT instruction or timestep exhaustion)

---

### WA-005: Assembly Round-Trip

```
[list[Instruction]]
        |
        v
[disassemble()] -- Instruction -> text lines with address prefixes
        |
        v
[Assembly Text (human-readable)]
        |
        v
[assemble()] -- text lines -> parse regex -> Instruction objects
        |
        v
[list[Instruction]]  (identical to original)
```

**Invariant:** `assemble(disassemble(program)) == program` for all valid programs (DA-012, BA-007)

---

### WA-006: Round-Trip Verification Workflow

```
[Train numpy MLP on synthetic data]
        |
        v
[Verify float accuracy > 90%]  (BA-025)
        |
        v
[compile_nn_to_spikecore(weights, biases, activations)]
        |
        v
[For 100 samples: compare PyTorch vs. SpikeCore top-1 predictions]
        |
        v
[Assert match rate >= 70%]  (BA-026)
        |
        v
[Verify assembly structure (HALT termination, ACC count)]  (BA-027)
        |
        v
[Verify non-empty spike log]  (BA-028)
```

---

### WA-007: Docker Build and Launch

```
[docker compose build]
        |
        +--> Stage 1 (tvm-builder):
        |        - Install system deps (cmake, llvm-15, ninja)
        |        - Clone TVM from github (main branch, depth 1)
        |        - cmake + ninja build
        |
        +--> Stage 2 (runtime):
                - Copy built TVM
                - Install Python deps from requirements.txt
                - Set TVM_HOME, PYTHONPATH, TVM_LIBRARY_PATH
                - Expose port 8888
                - CMD: jupyter notebook --token=spikecore

[docker compose up]
        |
        v
[Jupyter at http://localhost:8888/?token=spikecore]
        - notebooks/ mounted read-write
        - spikecore/ mounted read-write
        - tests/ mounted read-write
```

---

### Implicit Workflows (Identified Gaps)

| ID | Workflow | Status |
|----|---------|--------|
| IW-001 | Layer boundary detection during simulation | Implicit via ACC input range comparison; no explicit layer metadata |
| IW-002 | Core offset tracking across layers | Implicit via `_next_core_offset` in SpikeCoreCPU; no reset mechanism |
| IW-003 | Dual-path feature parity verification | No automated test compares TVM path output to standalone path output |

---

## 7. Deferred Work (Phase 2)

Items explicitly deferred to Phase 2 as documented in the design document and README:

| ID | Item | Description | Source |
|----|------|------------|--------|
| PH2-001 | MLIR Extension | `torch-mlir` -> custom `spikecore` MLIR dialect -> same simulator. Would add a `02_spikecore_mlir.ipynb` notebook. Deferred because it doubles build complexity. | DM-039 |
| PH2-002 | Convolutional Models | Event-driven convolutions for vision tasks (beyond the current MLP-only support). | DM-040 |
| PH2-003 | Multi-Chip Simulation | Spike routing between multiple SpikeCore chips. Current simulator is single-chip only. | DM-041 |
| PH2-004 | Power/Latency Estimation | Energy model based on spike counts and memory access patterns. No performance estimation exists currently. | DM-042 |
| PH2-005 | Lava Comparison | Side-by-side comparison with actual Intel Lava/Loihi workflow. | DM-043 |
| PH2-006 | Calibration Dataset | Use real calibration data for quantization scale estimation instead of `global_scale=8.0`. | DM-044 |

---

## 8. Traceability Matrix

### Legend

- **Spec ID**: User story identifier in this document
- **Source IDs**: Analyst report finding identifiers (FD=Feature Detective, DA=Domain Analyst, BA=Behavior Analyst, DM=Documentation Miner, WA=Workflow Analyst, IA=Infrastructure Analyst)
- **Code Location**: Primary source file(s)
- **Verification Level**: VERIFIED (test-backed), INFERRED (code analysis), DOCUMENTED (docs/comments)

| Spec ID | Source IDs | Code Location | Verification Level |
|---------|-----------|---------------|-------------------|
| US-1.1.1 | FD-001, DA-016, DA-017, BA-022, BA-023, BA-024 | `spikecore/byoc_codegen.py:155-215` | VERIFIED |
| US-1.1.2 | FD-001, DA-016, BA-022 | `spikecore/byoc_codegen.py:196-214` | VERIFIED |
| US-1.1.3 | FD-001, FD-002, DA-015, BA-022, BA-029, BA-030 | `spikecore/byoc_codegen.py:155-259` | VERIFIED |
| US-1.2.1 | FD-002, BA-029, BA-030 | `spikecore/byoc_codegen.py:218-259` | VERIFIED |
| US-1.3.1 | FD-003, DA-018, DA-019, DA-020 | `spikecore/byoc_codegen.py:95-114` | INFERRED |
| US-1.4.1 | FD-004, DA-018, DM-010 | `spikecore/byoc_codegen.py:117-148` | INFERRED |
| US-1.5.1 | FD-005, DA-013, DM-008 | `spikecore/relay_patterns.py:65-78` | INFERRED |
| US-1.5.2 | FD-006, DA-018 | `spikecore/byoc_codegen.py:54-92` | INFERRED |
| US-2.1.1 | FD-007, DA-021 | `spikecore/quantize.py:22-53` | INFERRED |
| US-2.2.1 | FD-008, DA-022, BA-019, BA-020 | `spikecore/quantize.py:56-78` | VERIFIED |
| US-2.3.1 | FD-009, DA-023 | `spikecore/quantize.py:81-104` | INFERRED |
| US-3.1.1 | FD-010, DA-001, DA-002, BA-009, BA-010, BA-012 | `spikecore/hardware_model.py:20-44` | VERIFIED |
| US-3.2.1 | FD-011, DA-001, DA-002, BA-011 | `spikecore/hardware_model.py:50-70` | VERIFIED |
| US-3.3.1 | FD-012, BA-012 | `spikecore/hardware_model.py:46-48` | VERIFIED |
| US-3.3.2 | FD-013, DA-006, BA-013 | `spikecore/hardware_model.py:72-84` | VERIFIED |
| US-3.3.3 | FD-013, DA-007, DA-008, DA-027, BA-014, BA-015 | `spikecore/hardware_model.py:86-108` | VERIFIED |
| US-3.3.4 | FD-013, DA-009, BA-016 | `spikecore/hardware_model.py:110-115` | VERIFIED |
| US-3.3.5 | FD-013, DA-003, DA-004, BA-018, BA-024 | `spikecore/hardware_model.py:168` | VERIFIED |
| US-3.3.6 | FD-013, DA-025, BA-017 | `spikecore/hardware_model.py:117-176` | VERIFIED |
| US-3.4.1 | FD-013, DA-005, DA-010, DA-025, DA-026, BA-014 | `spikecore/hardware_model.py:117-177` | VERIFIED |
| US-3.5.1 | FD-014, DA-026, BA-013 | `spikecore/hardware_model.py:183-193` | VERIFIED |
| US-4.1.1 | FD-016, DA-011, BA-001 | `spikecore/assembly.py:97-101` | VERIFIED |
| US-4.1.2 | FD-016, DA-011, BA-002, BA-003 | `spikecore/assembly.py:103-108` | VERIFIED |
| US-4.1.3 | FD-016, DA-011, BA-004 | `spikecore/assembly.py:110-113` | VERIFIED |
| US-4.1.4 | FD-016, DA-011, BA-005 | `spikecore/assembly.py:92-96` | VERIFIED |
| US-4.1.5 | FD-016, DA-011, BA-006 | `spikecore/assembly.py:85-86` | VERIFIED |
| US-4.1.6 | FD-016, DA-011, BA-006 | `spikecore/assembly.py:88-90` | VERIFIED |
| US-4.1.7 | FD-016, DA-011, BA-008 | `spikecore/assembly.py:115-128` | VERIFIED |
| US-4.2.1 | FD-017, DA-012, BA-007 | `spikecore/assembly.py:43-71` | VERIFIED |
| US-5.1.1 | FD-025, BA-025 | `tests/test_roundtrip.py:136-142` | VERIFIED |
| US-5.1.2 | FD-025, FD-026, DA-024, BA-026, DM-024 | `tests/test_roundtrip.py:144-179` | VERIFIED |
| US-5.1.3 | FD-026, BA-027 | `tests/test_roundtrip.py:181-195` | VERIFIED |
| US-5.1.4 | FD-026, BA-028 | `tests/test_roundtrip.py:197-220` | VERIFIED |
| US-6.1.1 | FD-018, DA-028 | `spikecore/visualize.py:17-67` | INFERRED |
| US-6.2.1 | FD-019, DA-029 | `spikecore/visualize.py:70-134` | INFERRED |
| US-6.3.1 | FD-020, DA-030 | `spikecore/visualize.py:137-172` | INFERRED |
| US-6.4.1 | FD-021, DA-031 | `spikecore/visualize.py:175-228` | INFERRED |
| US-7.1.1 | FD-022, DM-009 | `notebooks/01_spikecore_tvm.ipynb` Cell 2 | DOCUMENTED |
| US-7.1.2 | FD-022, DA-032, DM-004 | `notebooks/01_spikecore_tvm.ipynb` Cell 4 | DOCUMENTED |
| US-7.1.3 | FD-022, DA-022, DM-006 | `notebooks/01_spikecore_tvm.ipynb` Cells 6, 8 | DOCUMENTED |
| US-7.1.4 | FD-022, DA-025, DM-008, DM-009 | `notebooks/01_spikecore_tvm.ipynb` Cells 14-20 | DOCUMENTED |
| US-8.1.1 | FD-023, DM-002 | `Dockerfile` | DOCUMENTED |
| US-8.2.1 | FD-024, DM-009 | `docker-compose.yml` | DOCUMENTED |
| US-9.1.1 | FD-015, DA-003, DA-004, BA-001-005 | `spikecore/assembly.py:23-28` | VERIFIED |
| US-9.1.2 | FD-015, DA-003 | `spikecore/assembly.py:31-41` | INFERRED |

---

## 9. Coverage Assessment

### 9.1 Verification Level Distribution

| Level | Count | Percentage | Description |
|-------|-------|-----------|-------------|
| **VERIFIED** | 30 user stories | 68% | Backed by automated test assertions (BA-001 through BA-030) |
| **INFERRED** | 9 user stories | 21% | Derived from code analysis without direct test coverage |
| **DOCUMENTED** | 5 user stories | 11% | Supported by documentation only (README, design doc, notebook) |

**Total user stories:** 44

### 9.2 Feature Coverage by Analyst Source

| Feature Area | FD Features | Covered in Specs | Test Coverage |
|-------------|-------------|------------------|--------------|
| Neural Network Compilation | FD-001 to FD-004 | All 4 | FD-001, FD-002 VERIFIED; FD-003, FD-004 INFERRED |
| Relay Pattern Matching | FD-005, FD-006 | Both | INFERRED (0% test coverage on relay_patterns.py) |
| Model Quantization | FD-007 to FD-009 | All 3 | FD-008 VERIFIED; FD-007, FD-009 INFERRED |
| Hardware Simulation | FD-010 to FD-014 | All 5 | All VERIFIED |
| Instruction Set | FD-015 | Covered | VERIFIED |
| Assembly Language | FD-016, FD-017 | Both | All VERIFIED |
| Visualization | FD-018 to FD-021 | All 4 | All INFERRED (0% test coverage on visualize.py) |
| Notebook Walkthrough | FD-022 | Covered | DOCUMENTED |
| Docker Environment | FD-023, FD-024 | Both | DOCUMENTED |
| Testing | FD-025, FD-026 | Both | VERIFIED |

**All 26 features (FD-001 through FD-026) are represented in the specification.**

### 9.3 Behavior Analyst Coverage

All 30 verified behaviors (BA-001 through BA-030) are mapped to acceptance criteria:

| BA Range | Area | User Stories |
|----------|------|-------------|
| BA-001 to BA-008 | Assembly | US-4.1.1 through US-4.2.1 |
| BA-009 to BA-011 | NeuronCore | US-3.1.1, US-3.2.1 |
| BA-012 to BA-018 | SpikeCoreCPU | US-3.3.1 through US-3.3.6, US-3.4.1 |
| BA-019 to BA-021 | Quantization | US-2.2.1 |
| BA-022 to BA-024 | Codegen | US-1.1.1 through US-1.1.3 |
| BA-025 to BA-030 | Round-Trip | US-5.1.1 through US-5.1.4 |

### 9.4 Domain Rule Coverage

All 33 domain entities/rules (DA-001 through DA-033) are referenced in either the Entity Glossary, User Stories, or Non-Functional Requirements.

### 9.5 Gap Analysis

#### Critical Gaps (No Test Coverage)

| Gap | Impact | Source Files |
|-----|--------|-------------|
| `relay_patterns.py` has 0% test coverage | Pattern matching correctness is unverified | `spikecore/relay_patterns.py` |
| `visualize.py` has 0% test coverage | Visualization output correctness is unverified | `spikecore/visualize.py` |
| Multi-timestep simulation behavior is not tested | Only single-timestep tests exist; temporal dynamics untested | `spikecore/hardware_model.py:144-148` |
| FIRE shift computation not tested in simulator context | Shift is tested in codegen but not in hardware execution | `spikecore/hardware_model.py:100` |
| Activation bus clipping not tested | Bus values exceeding int16 range are untested | `spikecore/hardware_model.py:142` |
| Weight memory overflow not tested | Loading weights exceeding 256 bytes per core is untested | `spikecore/hardware_model.py:68` |
| NOP instruction has no handler in simulator | NOP falls through without explicit handling | `spikecore/hardware_model.py:155-168` |
| TVM-to-standalone equivalence not tested | No test verifies both paths produce identical assembly | WA-003 vs WA-002 |

#### Documentation Inconsistencies

| Issue | Details | Source |
|-------|---------|--------|
| int16 vs int32 accumulator inconsistency | Design doc and `hardware_model.py` docstring say "int16 accumulator"; README says "int32"; code uses `np.int32` | DM-030, DM-031 |
| Biases quantized but unused in simulation | `compile_nn_to_spikecore()` quantizes biases but the simulator never applies them | WA implicit |
| Cost estimator always returns 1 | No intelligence in offloading decisions; always prefers SpikeCore | DA-020 |
| HALT is semantically redundant | HALT only breaks the instruction loop; program would end naturally at the last instruction anyway | WA gap |

#### Quantitative Summary

- **Test files:** 3 (`test_simulator.py`, `test_codegen.py`, `test_roundtrip.py`)
- **Test classes:** 6
- **Test functions:** 22
- **Source files with 0% test coverage:** 2 of 6 (`relay_patterns.py`, `visualize.py`)
- **Source files with partial coverage:** 2 of 6 (`byoc_codegen.py` TVM path untested, `quantize.py` TVM path untested)
- **Source files with strong coverage:** 2 of 6 (`assembly.py`, `hardware_model.py`)
- **TODO/FIXME/HACK markers in code:** 0
- **Phase 2 items deferred:** 6

---

*End of Derived Specification*
